{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MWEvfgVXOzO"
      },
      "source": [
        "# Weights and Biases\n",
        "\n",
        "Notebook with code from [https://theaisummer.com/weights-and-biases-tutorial/](https://theaisummer.com/weights-and-biases-tutorial/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define example net\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydpqIQ1eWnG-"
      },
      "outputs": [],
      "source": [
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uX0RugDFWdVg"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose(\n",
        "                [transforms.ToTensor(), \n",
        "                 transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, fc_layer_size=84):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, fc_layer_size)\n",
        "        self.fc3 = nn.Linear(fc_layer_size, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jl0GZTxdYF0D"
      },
      "outputs": [],
      "source": [
        "# set up device\n",
        "if torch.cuda.is_available():\n",
        " dev = \"cuda:0\"\n",
        "else:\n",
        " dev = \"cpu\"\n",
        "print(dev)\n",
        "device = torch.device(dev)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOaZ_BbNWzf3"
      },
      "outputs": [],
      "source": [
        "# init wandb\n",
        "run = wandb.init(project='test')\n",
        "\n",
        "# set config\n",
        "config = wandb.config\n",
        "config.learning_rate = 0.01\n",
        "config.epochs = 10\n",
        "\n",
        "# instatiate net\n",
        "net = Net()\n",
        "net.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(net.parameters(),lr=config.learning_rate)\n",
        "\n",
        "# train\n",
        "for epoch in range(config.epochs):\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            wandb.log({'epoch': epoch+1, 'loss': running_loss/2000})\n",
        "            wandb.watch(net, criterion, log=\"all\")\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMtNvTo4XJX9"
      },
      "source": [
        "# Visualization\n",
        "\n",
        "uses the train model, to visualize the training process and send it to W&B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQaootT9XIko"
      },
      "outputs": [],
      "source": [
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "columns=['image','label']\n",
        "data = []\n",
        "\n",
        "for i, batch in enumerate(trainloader, 0):\n",
        "    inputs, labels = batch[0], batch[1]\n",
        "    for j, image in enumerate(inputs,0):\n",
        "        data.append([wandb.Image(image),classes[labels[j].item()]])\n",
        "    break\n",
        "\n",
        "table= wandb.Table(data=data, columns=columns)\n",
        "run.log({\"cifar10_images\": table})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVzPVuwTXyOs"
      },
      "source": [
        "# Artifacts\n",
        "\n",
        "versioncontrol on your data by registering in as an artifact"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fZJHCOaXxtU"
      },
      "outputs": [],
      "source": [
        "cifar10_artifact = wandb.Artifact(\"cifar10\", type=\"dataset\")\n",
        "file_path = './data/cifar-10-batches-py'\n",
        "cifar10_artifact.add_dir(file_path)\n",
        "run.log_artifact(cifar10_artifact)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6NCOMtBW1r5"
      },
      "source": [
        "# Sweeps\n",
        "\n",
        "like doing a grid search on your runs\n",
        "\n",
        "either a range of values or different values\n",
        "\n",
        "\"a sweep that tries all combinations to find the best hyperparameters for your model.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NkipudfXzDy"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "sweep_config = {\n",
        "                'method': 'random',\n",
        "                'metric': {'goal': 'minimize', 'name': 'loss'},\n",
        "                'parameters': {\n",
        "                    'batch_size': {\n",
        "                        'distribution': 'q_log_uniform_values',\n",
        "                        'max': 256,\n",
        "                        'min': 32 },\n",
        "                    'epochs': {'value': 5},\n",
        "                    'fc_layer_size': {'values': [128, 256, 512]},\n",
        "                    'learning_rate': {'distribution': 'uniform',\n",
        "                                      'max': 0.1,\n",
        "                                      'min': 0},\n",
        "                    'optimizer': {'values': ['adam', 'sgd']}\n",
        "                }\n",
        " }\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BN3CWlVcW3PS"
      },
      "outputs": [],
      "source": [
        "def train(config=None):\n",
        "\n",
        "    with wandb.init(project='test', entity='serkar', config=config):\n",
        "        config = wandb.config\n",
        "\n",
        "        transform = transforms.Compose(\n",
        "            [transforms.ToTensor(),\n",
        "             transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "        \n",
        "        trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "        trainloader = torch.utils.data.DataLoader(trainset, batch_size=config.batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "        testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "        net = Net(config.fc_layer_size)\n",
        "        net.to(device)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = torch.optim.SGD(net.parameters(), lr=config.learning_rate)\n",
        "\n",
        "        if config.optimizer == \"sgd\":\n",
        "            optimizer = torch.optim.SGD(net.parameters(), lr=config.learning_rate, momentum=0.9)\n",
        "\n",
        "        elif optimizer == \"adam\":\n",
        "            optimizer = torch.optim.Adam(net.parameters(), lr=config.learning_rate)\n",
        "\n",
        "        wandb.watch(net, criterion, log=\"all\")\n",
        "\n",
        "        for epoch in range(config.epochs):  # loop over the dataset multiple times\n",
        "\n",
        "            running_loss = 0.0\n",
        "\n",
        "            for i, data in enumerate(trainloader, 0):\n",
        "\n",
        "                inputs, labels = data[0].to(device), data[1].to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = net(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                running_loss += loss.item()\n",
        "\n",
        "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / len(trainloader)))\n",
        "\n",
        "            wandb.log({'epoch': epoch + 1, 'loss': running_loss / len(trainloader)})\n",
        "\n",
        "        print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQFsqtDNXDWH"
      },
      "outputs": [],
      "source": [
        "# run sweep\n",
        "wandb.agent(sweep_id, function=train, count=5)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
