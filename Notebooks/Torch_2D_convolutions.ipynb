{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "structured-tuning",
      "metadata": {
        "id": "structured-tuning"
      },
      "source": [
        "# PyTorch 2D convolutions\n",
        "#### Advanced Deep Learning, 2024"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ooBCszdvpKZs",
      "metadata": {
        "id": "ooBCszdvpKZs"
      },
      "source": [
        "## mount driver and establish workspace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "F3LgNo85paJH",
      "metadata": {
        "id": "F3LgNo85paJH"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "realistic-banana",
      "metadata": {
        "id": "realistic-banana"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sized-craft",
      "metadata": {
        "id": "sized-craft"
      },
      "source": [
        "## One input channel, one output, no padding\n",
        "Let's define a `W`$\\times$`W` filter. For the following examples, we do not need a bias parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "capital-granny",
      "metadata": {
        "id": "capital-granny"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We just defined: Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), bias=False)\n"
          ]
        }
      ],
      "source": [
        "W = 3 # size of convolution filter\n",
        "# 1 input (image) channel, 1 output channel, WxW convolution kernel\n",
        "conv = nn.Conv2d(1, 1, W, bias=False)\n",
        "print(\"We just defined:\", conv)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "virgin-throat",
      "metadata": {
        "id": "virgin-throat"
      },
      "source": [
        "Let's look at the kernel dimensions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "homeless-discovery",
      "metadata": {
        "id": "homeless-discovery",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 1, 3, 3])\n"
          ]
        }
      ],
      "source": [
        "# 1 output channel, 1 input channel, 1st dimension = W, 2nd dimension = W\n",
        "print(conv.weight.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ordered-working",
      "metadata": {
        "id": "ordered-working"
      },
      "source": [
        "The filter parameters are initialized randomly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "excess-pennsylvania",
      "metadata": {
        "id": "excess-pennsylvania"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[[[-0.2544, -0.0415,  0.3305],\n",
            "          [ 0.0182,  0.1216,  0.0556],\n",
            "          [-0.0222, -0.0012, -0.1553]]]], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "print(conv.weight)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lonely-monaco",
      "metadata": {
        "id": "lonely-monaco"
      },
      "source": [
        "We can set the parameters as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ambient-sender",
      "metadata": {
        "id": "ambient-sender"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[[[1., 1., 1.],\n",
            "          [1., 1., 1.],\n",
            "          [1., 1., 1.]]]], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "conv.weight = torch.nn.Parameter(torch.ones_like(conv.weight))\n",
        "print(conv.weight)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "serial-evolution",
      "metadata": {
        "id": "serial-evolution"
      },
      "source": [
        "Let's define an input (image) `x`. The input is of the same shape as the filter:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "preceding-spank",
      "metadata": {
        "id": "preceding-spank",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input:\n",
            " tensor([[[[0., 1., 2.],\n",
            "          [3., 4., 5.],\n",
            "          [6., 7., 8.]]]])\n",
            "Sum of all input elements: 36.0\n"
          ]
        }
      ],
      "source": [
        "x = torch.arange(float(W*W))\n",
        "x = torch.reshape(x, (1, 1, W, W))\n",
        "print('Input:\\n', x)\n",
        "print('Sum of all input elements:', torch.sum(x).item())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "streaming-benjamin",
      "metadata": {
        "id": "streaming-benjamin"
      },
      "source": [
        "Because there is no padding and input and filter have the same size, there is only one valid position for the filter. Accordingly, the result is a tensor with a single value:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "million-bunch",
      "metadata": {
        "id": "million-bunch"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensor: tensor([[[[36.]]]], grad_fn=<ConvolutionBackward0>) scalar: 36.0\n"
          ]
        }
      ],
      "source": [
        "c = conv(x)\n",
        "print('Tensor:', c, 'scalar:', c.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "final-positive",
      "metadata": {
        "id": "final-positive"
      },
      "source": [
        "The scalar should be equal to the sum of all input elements (ensure that you understand why)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "limiting-stomach",
      "metadata": {
        "id": "limiting-stomach"
      },
      "source": [
        "## One input channel, one output,  padding\n",
        "Now we add zero-padding such that the input dimensionality is preseved:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "naughty-alfred",
      "metadata": {
        "id": "naughty-alfred"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[[ 8., 15., 12.],\n",
            "          [21., 36., 27.],\n",
            "          [20., 33., 24.]]]], grad_fn=<ConvolutionBackward0>)\n"
          ]
        }
      ],
      "source": [
        "conv = nn.Conv2d(1, 1, W, padding=W//2, bias=False)\n",
        "conv.weight = torch.nn.Parameter(torch.ones_like(conv.weight))\n",
        "c = conv(x)\n",
        "print(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "invalid-communication",
      "metadata": {
        "id": "invalid-communication"
      },
      "source": [
        "## Several input channels, one output, no padding\n",
        "Typically, the input to a convolutional layer consists of several feature maps or channels. For example, consider a 2D input with three channels (e.g., an RGB colour image):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "amber-somerset",
      "metadata": {
        "id": "amber-somerset"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input: tensor([[[[ 0.,  1.,  2.],\n",
            "          [ 3.,  4.,  5.],\n",
            "          [ 6.,  7.,  8.]],\n",
            "\n",
            "         [[ 9., 10., 11.],\n",
            "          [12., 13., 14.],\n",
            "          [15., 16., 17.]],\n",
            "\n",
            "         [[18., 19., 20.],\n",
            "          [21., 22., 23.],\n",
            "          [24., 25., 26.]]]])\n",
            "Sum of all inputs: 351.0\n"
          ]
        }
      ],
      "source": [
        "x = torch.arange(float(3*W*W))\n",
        "x = torch.reshape(x, (1, 3, W, W))\n",
        "print('Input:', x)\n",
        "print('Sum of all inputs:', torch.sum(x).item())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "voluntary-porter",
      "metadata": {
        "id": "voluntary-porter"
      },
      "source": [
        "Let's define a convolutional layer that takes three channels as input and produces a single output feature map:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "spare-ranch",
      "metadata": {
        "id": "spare-ranch"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Weight parameters of convolutional layer: Parameter containing:\n",
            "tensor([[[[1., 1., 1.],\n",
            "          [1., 1., 1.],\n",
            "          [1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.],\n",
            "          [1., 1., 1.],\n",
            "          [1., 1., 1.]],\n",
            "\n",
            "         [[1., 1., 1.],\n",
            "          [1., 1., 1.],\n",
            "          [1., 1., 1.]]]], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "# 3 input (image) channels, 1 output channel, WxW convolution kernel\n",
        "conv = nn.Conv2d(3, 1, W, bias=False)\n",
        "conv.weight = torch.nn.Parameter(torch.ones_like(conv.weight))\n",
        "print('Weight parameters of convolutional layer:', conv.weight)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "prime-wrapping",
      "metadata": {
        "id": "prime-wrapping"
      },
      "source": [
        "Note that there is one filter for each input channel.\n",
        "The convolutional layer first convolves each input channel with the corresponding filter.\n",
        "This results in three feature maps, whih are added to give the final result:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "remarkable-prayer",
      "metadata": {
        "id": "remarkable-prayer",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of filter parameters: 27 \n",
            "result of filtering the input: tensor([[[[351.]]]], grad_fn=<ConvolutionBackward0>)\n"
          ]
        }
      ],
      "source": [
        "c = conv(x)\n",
        "print('number of filter parameters:', conv.weight.numel(), '\\nresult of filtering the input:', c)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "accessible-calcium",
      "metadata": {
        "id": "accessible-calcium"
      },
      "source": [
        "It is important that the number of parameters and the dimesionality of the result is clear to you."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "expected-antarctica",
      "metadata": {
        "id": "expected-antarctica"
      },
      "source": [
        "Now let's apply 1$\\times$1 convolutions to our three input channels. Again, we set all filter weights to 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "likely-front",
      "metadata": {
        "id": "likely-front",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[[[1.]],\n",
            "\n",
            "         [[1.]],\n",
            "\n",
            "         [[1.]]]], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "# 3 input (image) channels, 1 output channel, 1x1 convolution kernel\n",
        "conv = nn.Conv2d(3, 1, 1, bias=False)\n",
        "conv.weight = torch.nn.Parameter(torch.ones_like(conv.weight))\n",
        "print(conv.weight)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "advisory-resort",
      "metadata": {
        "id": "advisory-resort"
      },
      "source": [
        "This convolutional layer adds the three input feature maps/channels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "amateur-sharing",
      "metadata": {
        "id": "amateur-sharing",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[[27., 30., 33.],\n",
            "          [36., 39., 42.],\n",
            "          [45., 48., 51.]]]], grad_fn=<ConvolutionBackward0>)\n"
          ]
        }
      ],
      "source": [
        "c = conv(x)\n",
        "print(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "million-liberty",
      "metadata": {
        "id": "million-liberty"
      },
      "source": [
        "Thus, 1$\\times$1 convolutions can be used to compute weighted sums of input feature maps/channels (in our previous example, all weights were set to 1)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "threatened-rolling",
      "metadata": {
        "id": "threatened-rolling"
      },
      "source": [
        "## Several output maps\n",
        "Typically, convolutional layer produce several feature maps or channels. For example, consider\n",
        "extending the previous 1$\\times$1 example to two output maps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "effective-reform",
      "metadata": {
        "id": "effective-reform",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[[[1.]],\n",
            "\n",
            "         [[1.]],\n",
            "\n",
            "         [[1.]]],\n",
            "\n",
            "\n",
            "        [[[1.]],\n",
            "\n",
            "         [[1.]],\n",
            "\n",
            "         [[1.]]]], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "# 3 input (image) channels, 2 output channel, 1x1 convolution kernel\n",
        "conv = nn.Conv2d(3, 2, 1, bias=False)\n",
        "conv.weight = torch.nn.Parameter(torch.ones_like(conv.weight))\n",
        "print(conv.weight)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "veterinary-while",
      "metadata": {
        "id": "veterinary-while"
      },
      "source": [
        "This layer maps 3 input feature maps to 2 output feature maps, which are identical in our example, because we initialized all filters so that they are identical:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "civilian-dallas",
      "metadata": {
        "id": "civilian-dallas",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[[27., 30., 33.],\n",
            "          [36., 39., 42.],\n",
            "          [45., 48., 51.]],\n",
            "\n",
            "         [[27., 30., 33.],\n",
            "          [36., 39., 42.],\n",
            "          [45., 48., 51.]]]], grad_fn=<ConvolutionBackward0>)\n"
          ]
        }
      ],
      "source": [
        "c = conv(x)\n",
        "print(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "korean-arrow",
      "metadata": {
        "id": "korean-arrow"
      },
      "source": [
        "The first convolutional layer in a network has typically more output feature maps than input channels. Let's assume 3 input channels, 4 output channels of the same dimensionality (i.e., we use padding), and a filter size of 3. For each output channel, we have 3 filter with 9 parameters/weights each. Thus, we have 108 parameters in total:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "infrared-renewal",
      "metadata": {
        "id": "infrared-renewal",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[[[ 0.0863, -0.1034,  0.1450],\n",
            "          [-0.1824, -0.1236, -0.0497],\n",
            "          [-0.0248,  0.0069, -0.0047]],\n",
            "\n",
            "         [[ 0.0358,  0.1653,  0.1725],\n",
            "          [ 0.1403, -0.1190,  0.0515],\n",
            "          [ 0.0837,  0.0609, -0.0186]],\n",
            "\n",
            "         [[-0.1076,  0.0920,  0.1810],\n",
            "          [ 0.0821,  0.1350, -0.1471],\n",
            "          [-0.1687,  0.0727, -0.1163]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0442,  0.1266, -0.0284],\n",
            "          [ 0.0501, -0.0818, -0.1503],\n",
            "          [ 0.1227, -0.1921,  0.1739]],\n",
            "\n",
            "         [[-0.0906, -0.1724,  0.1704],\n",
            "          [-0.1030,  0.0147,  0.1574],\n",
            "          [ 0.0290,  0.0748,  0.1856]],\n",
            "\n",
            "         [[ 0.1392,  0.0631, -0.0279],\n",
            "          [-0.0229, -0.0592,  0.0458],\n",
            "          [ 0.1494, -0.0300,  0.1382]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1242, -0.0567, -0.1043],\n",
            "          [-0.0937, -0.1011,  0.1120],\n",
            "          [-0.1193, -0.0751,  0.0639]],\n",
            "\n",
            "         [[-0.1208,  0.1147,  0.1707],\n",
            "          [ 0.1494, -0.1350,  0.0432],\n",
            "          [ 0.0190,  0.0149, -0.0318]],\n",
            "\n",
            "         [[ 0.0370, -0.0874, -0.1871],\n",
            "          [-0.1367,  0.1644, -0.1639],\n",
            "          [ 0.0758,  0.0835,  0.0033]]],\n",
            "\n",
            "\n",
            "        [[[-0.0366, -0.1849,  0.0139],\n",
            "          [ 0.1280,  0.0386,  0.0774],\n",
            "          [-0.1242, -0.1111, -0.0336]],\n",
            "\n",
            "         [[-0.1125, -0.0115,  0.0669],\n",
            "          [ 0.1343,  0.0596, -0.1633],\n",
            "          [-0.1030, -0.0075,  0.0995]],\n",
            "\n",
            "         [[-0.1216, -0.1474,  0.0305],\n",
            "          [ 0.1110,  0.0979,  0.0407],\n",
            "          [-0.0526, -0.0719,  0.0089]]]], requires_grad=True)\n",
            "Number of parameters: 108\n"
          ]
        }
      ],
      "source": [
        "conv = nn.Conv2d(3, 4, W, padding=W//2, bias=False)\n",
        "print(conv.weight)\n",
        "print(\"Number of parameters:\", conv.weight.shape.numel())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "primary-chair",
      "metadata": {
        "id": "primary-chair"
      },
      "source": [
        "And here are the resulting feature maps when applied to our input:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "blond-damages",
      "metadata": {
        "id": "blond-damages"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[[-1.5118, -1.6374,  3.7590],\n",
            "          [ 6.1848,  4.8867,  5.4996],\n",
            "          [ 7.9212,  9.7276,  5.8731]],\n",
            "\n",
            "         [[ 7.2011,  9.9922,  0.9502],\n",
            "          [ 8.7622, 14.0511,  2.2081],\n",
            "          [ 2.0452,  2.2778, -2.0802]],\n",
            "\n",
            "         [[ 0.7951,  1.1204,  3.5968],\n",
            "          [-1.7630, -2.3250,  2.1555],\n",
            "          [-4.2507, -6.3818, -1.4570]],\n",
            "\n",
            "         [[ 0.9393,  1.4685,  0.9650],\n",
            "          [-0.4497, -3.0735, -5.6535],\n",
            "          [ 0.2295,  1.5823,  0.9298]]]], grad_fn=<ConvolutionBackward0>)\n"
          ]
        }
      ],
      "source": [
        "c = conv(x)\n",
        "print(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "central-ozone",
      "metadata": {
        "id": "central-ozone"
      },
      "source": [
        "# Image processing examples\n",
        "Now we consider a more complex example that involves some basic image transformations. First, we need to import NumPy and some image utilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "preliminary-discount",
      "metadata": {
        "id": "preliminary-discount"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "concerned-demographic",
      "metadata": {
        "id": "concerned-demographic"
      },
      "source": [
        "Let's load an image and convert it to grayscale so that we just deal with a single channel. The image is available from Absalon. Please upload it to your google drive folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "surprised-advancement",
      "metadata": {
        "id": "surprised-advancement"
      },
      "outputs": [],
      "source": [
        "image = Image.open('diku.jpg')  # Load image\n",
        "image = torchvision.transforms.functional.to_grayscale(image)  # Transform to grayscale, because we only wnat one channel"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "colonial-netscape",
      "metadata": {
        "id": "colonial-netscape"
      },
      "source": [
        "Let's plot the image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "median-coordinate",
      "metadata": {
        "id": "median-coordinate"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PIL image shape: (42, 134) min: 0 max: 255\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "img_np = np.asarray(image)\n",
        "print(\"PIL image shape:\", img_np.shape, \"min:\", img_np.min(), \"max:\", img_np.max())\n",
        "plt.imshow(image, cmap='gray', vmin=0, vmax=255)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "broadband-lingerie",
      "metadata": {
        "id": "broadband-lingerie"
      },
      "source": [
        "The transformation of the image to a tensor maps has two important effects. First, the values are rescaled to $[0.,1.]$. Second, the channels become the first dimension.  The latter implies that, if we want to plot the image, we have to reorder the axes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "caroline-strength",
      "metadata": {
        "id": "caroline-strength"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensor shape: torch.Size([1, 42, 134]) min: 0.0 max: 1.0\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "x = torchvision.transforms.ToTensor()(image)\n",
        "print(\"Tensor shape:\", x.shape, \"min:\", x.min().item(), \"max:\", x.max().item())\n",
        "plt.imshow(x.permute(1, 2, 0).squeeze(), cmap='gray', vmin=0, vmax=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "suitable-drill",
      "metadata": {
        "id": "suitable-drill"
      },
      "source": [
        "In order to be process by a layer, the tensor needs  another dimension/axis for enumerating the elements in a batch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "heated-service",
      "metadata": {
        "id": "heated-service"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape after adding batch dimension: torch.Size([1, 1, 42, 134])\n"
          ]
        }
      ],
      "source": [
        "x.unsqueeze_(0)  # Add a dimension\n",
        "print(\"Shape after adding batch dimension:\", x.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "empirical-rainbow",
      "metadata": {
        "id": "empirical-rainbow"
      },
      "source": [
        "Now we apply a simple horizontal gradient filter:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "trained-eligibility",
      "metadata": {
        "id": "trained-eligibility"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Kernel: tensor([[[[-1.,  1.]]]]) shape: torch.Size([1, 1, 1, 2])\n",
            "Tensor shape: torch.Size([1, 1, 42, 135]) min: -1.0 max: 1.0\n"
          ]
        }
      ],
      "source": [
        "hf = torch.tensor([[[[-1., 1.]]]])  # Define filter\n",
        "print(\"Kernel:\", hf, \"shape:\", hf.shape)\n",
        "\n",
        "conv = nn.Conv2d(1, 1, kernel_size=(1, 2), padding=(0, 1), bias=False)  # Padding only in one dimension needed\n",
        "conv.weight = torch.nn.Parameter(hf, requires_grad=False)  # Set kernel parameters to predefined filter parameters\n",
        "c = conv(x)  # Apply filter\n",
        "print(\"Tensor shape:\", c.shape, \"min:\", c.min().item(), \"max:\", c.max().item())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "historic-comedy",
      "metadata": {
        "id": "historic-comedy"
      },
      "source": [
        "We do not need a gradient for the kernel parameters, so we can use ``requires_grad=False``. This allows us to use ``c[0.0]`` as a NumPy array in the visualizaiton below. Alternatively, we could use ``c[0,0].detach()`` in the ``imshow`` call."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "natural-trail",
      "metadata": {
        "id": "natural-trail",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([42, 135])\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(c[0,0].shape)\n",
        "plt.imshow(c[0,0], cmap='gray', vmin=-1, vmax=1)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
